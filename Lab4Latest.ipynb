{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Four: Evaluation and Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arely Alcantara, Emily Fashenpour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we selected is titled \"Wine Quality\" and it looks at a total of 6497 samples of both red and white wines - where there is a CSV file for red and white wines respectively. The dataset looks at 11 attributes of each wine sample such as the acidity, amount of alcohol, and density just to name a few. Each wine sample was also given a quality value that rates how good or bad that wine is on a numerical scale between 1 and 10. We take these quality values and define a range for either bad, ok, or great wines. Given a new wine company entering the market, we hope to be able to classify and find what its quality is relative to the other existing samples in our dataset. Third parties interested in this information might be new companies or people trying to enter the wine market and seeing how their creation relates to existing wines in the market and how its quality might be in relation to existing wineries. More and more people are now trying to start their own wineries as a side job - take Dylan Sprouse for instance, you may know him from his role in The Suite Life of Zack and Cody - he's an actor and yet he has his own winery in New York! (2)\n",
    "We could essentially deploy this solution and charge a small fee to those companies/individuals wanting to find out what their wine quality is before they launch themselves into the market. In analyzing this dataset, we hope to help different wineries obtain a quality value for their wine creation and take into account the same attributes that our dataset uses.\n",
    "- Dataset URL: https://www.kaggle.com/danielpanizzo/wine-quality\n",
    "- Classification task: Classify a wine's quality as either bad, ok, or great quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to read in both of our CSV files and combine them to have one central source of information - we also added a color attribute, so that we know what each wine sample refers to in terms of red vs white wines. We're also going to remove column(s) that we feel aren't necessary for our analysis. We removed an index column that was included in both CSVs since we do not really care what a sample's position is in the dataset. Other than that, we feel like all of the other attributes are critical to determining the quality of a wine sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.043</td>\n",
       "      <td>19.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.99444</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.42</td>\n",
       "      <td>11.4</td>\n",
       "      <td>6</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.021</td>\n",
       "      <td>24.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.98965</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.61</td>\n",
       "      <td>12.4</td>\n",
       "      <td>9</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.036</td>\n",
       "      <td>24.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.99470</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.7</td>\n",
       "      <td>6</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>39.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.99226</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.39</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.40</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.052</td>\n",
       "      <td>81.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.69</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.5              0.38         0.33             9.2      0.043   \n",
       "1            6.6              0.36         0.29             1.6      0.021   \n",
       "2            8.5              0.21         0.41             4.3      0.036   \n",
       "3            5.8              0.23         0.20             2.0      0.043   \n",
       "4            6.9              0.24         0.40            15.4      0.052   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                 19.0                 116.0  0.99444  3.08       0.42   \n",
       "1                 24.0                  85.0  0.98965  3.41       0.61   \n",
       "2                 24.0                  99.0  0.99470  3.18       0.53   \n",
       "3                 39.0                 154.0  0.99226  3.21       0.39   \n",
       "4                 81.0                 198.0  0.99860  3.20       0.69   \n",
       "\n",
       "   alcohol  quality  color  \n",
       "0     11.4        6  White  \n",
       "1     12.4        9  White  \n",
       "2      9.7        6  White  \n",
       "3     10.2        6  White  \n",
       "4      9.4        5  White  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#read red and white wine csv files respectively\n",
    "redWines = pd.read_csv('wine-quality/wineQualityReds.csv')\n",
    "whiteWines = pd.read_csv('wine-quality/wineQualityWhites.csv')\n",
    "#add a color attribute, so we can differentiate between red and white wines\n",
    "redWines['color']='Red'\n",
    "whiteWines['color']='White'\n",
    "#add both csv files so that there is only one dataframe\n",
    "winesDf = pd.concat([redWines, whiteWines], ignore_index=True)\n",
    "\n",
    "#drop unneeded columns\n",
    "winesDf.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "#rename some columns - from periods to underscores to indicate spaces\n",
    "winesDf = winesDf.rename(columns = {'fixed.acidity': 'fixed_acidity', 'volatile.acidity': 'volatile_acidity', 'citric.acid':'citric_acid', 'residual.sugar':'residual_sugar', 'free.sulfur.dioxide':'free_sulfur_dioxide', 'total.sulfur.dioxide': 'total_sulfur_dioxide'})\n",
    "\n",
    "#shuffle rows and reset indices\n",
    "winesDf = winesDf.sample(frac=1).reset_index(drop=True)\n",
    "winesDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 13 columns):\n",
      "fixed_acidity           6497 non-null float64\n",
      "volatile_acidity        6497 non-null float64\n",
      "citric_acid             6497 non-null float64\n",
      "residual_sugar          6497 non-null float64\n",
      "chlorides               6497 non-null float64\n",
      "free_sulfur_dioxide     6497 non-null float64\n",
      "total_sulfur_dioxide    6497 non-null float64\n",
      "density                 6497 non-null float64\n",
      "pH                      6497 non-null float64\n",
      "sulphates               6497 non-null float64\n",
      "alcohol                 6497 non-null float64\n",
      "quality                 6497 non-null int64\n",
      "color                   6497 non-null object\n",
      "dtypes: float64(11), int64(1), object(1)\n",
      "memory usage: 660.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#show current column info with data type\n",
    "print(winesDf.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most of the attributes are floats since most of the measures such as acidity, sugar, sulphates - are all numerical values that are needed to predict the quality of a wine sample - so, we have decided not to alter these values so that we can use all attributes and use those to classify a quality value. Later on we're going to change the quality ranges to 3 classes of bad, ok, and good. Where values of 1 to 4 are classified as 'bad' wines, 5 to 7 are 'ok' wines, and 8 to 10 are 'good' wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 13 columns):\n",
      "fixed_acidity           6497 non-null float64\n",
      "volatile_acidity        6497 non-null float64\n",
      "citric_acid             6497 non-null float64\n",
      "residual_sugar          6497 non-null float64\n",
      "chlorides               6497 non-null float64\n",
      "free_sulfur_dioxide     6497 non-null float64\n",
      "total_sulfur_dioxide    6497 non-null float64\n",
      "density                 6497 non-null float64\n",
      "pH                      6497 non-null float64\n",
      "sulphates               6497 non-null float64\n",
      "alcohol                 6497 non-null float64\n",
      "quality                 6497 non-null int64\n",
      "color                   6497 non-null int64\n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 660.0 KB\n"
     ]
    }
   ],
   "source": [
    "#change color to be an int\n",
    "winesDf['color']=winesDf['color'].map({'Red': 1, 'White': 2})\n",
    "\n",
    "winesDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>7.215307</td>\n",
       "      <td>0.339666</td>\n",
       "      <td>0.318633</td>\n",
       "      <td>5.443235</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>30.525319</td>\n",
       "      <td>115.744574</td>\n",
       "      <td>0.994697</td>\n",
       "      <td>3.218501</td>\n",
       "      <td>0.531268</td>\n",
       "      <td>10.491801</td>\n",
       "      <td>5.818378</td>\n",
       "      <td>1.753886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.296434</td>\n",
       "      <td>0.164636</td>\n",
       "      <td>0.145318</td>\n",
       "      <td>4.757804</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>17.749400</td>\n",
       "      <td>56.521855</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.160787</td>\n",
       "      <td>0.148806</td>\n",
       "      <td>1.192712</td>\n",
       "      <td>0.873255</td>\n",
       "      <td>0.430779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.992340</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.996990</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "count    6497.000000       6497.000000  6497.000000     6497.000000   \n",
       "mean        7.215307          0.339666     0.318633        5.443235   \n",
       "std         1.296434          0.164636     0.145318        4.757804   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.400000          0.230000     0.250000        1.800000   \n",
       "50%         7.000000          0.290000     0.310000        3.000000   \n",
       "75%         7.700000          0.400000     0.390000        8.100000   \n",
       "max        15.900000          1.580000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free_sulfur_dioxide  total_sulfur_dioxide      density  \\\n",
       "count  6497.000000          6497.000000           6497.000000  6497.000000   \n",
       "mean      0.056034            30.525319            115.744574     0.994697   \n",
       "std       0.035034            17.749400             56.521855     0.002999   \n",
       "min       0.009000             1.000000              6.000000     0.987110   \n",
       "25%       0.038000            17.000000             77.000000     0.992340   \n",
       "50%       0.047000            29.000000            118.000000     0.994890   \n",
       "75%       0.065000            41.000000            156.000000     0.996990   \n",
       "max       0.611000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality        color  \n",
       "count  6497.000000  6497.000000  6497.000000  6497.000000  6497.000000  \n",
       "mean      3.218501     0.531268    10.491801     5.818378     1.753886  \n",
       "std       0.160787     0.148806     1.192712     0.873255     0.430779  \n",
       "min       2.720000     0.220000     8.000000     3.000000     1.000000  \n",
       "25%       3.110000     0.430000     9.500000     5.000000     2.000000  \n",
       "50%       3.210000     0.510000    10.300000     6.000000     2.000000  \n",
       "75%       3.320000     0.600000    11.300000     6.000000     2.000000  \n",
       "max       4.010000     2.000000    14.900000     9.000000     2.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winesDf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6497 entries and every field is filled - therefore we have no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Description</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Discrete/Continuous</th>\n",
       "      <th>Range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fixed_acidity</td>\n",
       "      <td>does not evaporate readily</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>3.8 - 15.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>volatile_acidity</td>\n",
       "      <td>the amount of acetic acid in wine</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>0.08 - 1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>citric_acid</td>\n",
       "      <td>citric acid can add freshness and flavor to wines</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>0 - 1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>residual_sugar</td>\n",
       "      <td>sugar remaining after fermentation stops</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>0.66 - 65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>chlorides</td>\n",
       "      <td>the amount of salt in the wine</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>0.01 - 0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>free_sulfur_dioxide</td>\n",
       "      <td>prevents microbial growth and the oxidation</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1 - 289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>total_sulfur_dioxide</td>\n",
       "      <td>SO2 becomes evident in the nose and taste of wine</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>6 - 440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>density</td>\n",
       "      <td>water depending on the percent alcohol and sug...</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>0.99 - 1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>pH</td>\n",
       "      <td>describes how acidic or basic a wine is</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>2.72 - 4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>sulphates</td>\n",
       "      <td>sulfur dioxide gas (S02) levels</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>0.22 -2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>the percent alcohol content of the wine</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>8 - 14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>quality</td>\n",
       "      <td>value given by experts on wine quality</td>\n",
       "      <td>nominal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>0: Bad, 1: OK, 2: Great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>color</td>\n",
       "      <td>refers to color of wine</td>\n",
       "      <td>nominal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1: Red, 2: White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Features                                        Description  \\\n",
       "0          fixed_acidity                         does not evaporate readily   \n",
       "1       volatile_acidity                  the amount of acetic acid in wine   \n",
       "2            citric_acid  citric acid can add freshness and flavor to wines   \n",
       "3         residual_sugar           sugar remaining after fermentation stops   \n",
       "4              chlorides                     the amount of salt in the wine   \n",
       "5    free_sulfur_dioxide        prevents microbial growth and the oxidation   \n",
       "6   total_sulfur_dioxide  SO2 becomes evident in the nose and taste of wine   \n",
       "7                density  water depending on the percent alcohol and sug...   \n",
       "8                     pH            describes how acidic or basic a wine is   \n",
       "9              sulphates                    sulfur dioxide gas (S02) levels   \n",
       "10               alcohol            the percent alcohol content of the wine   \n",
       "11               quality             value given by experts on wine quality   \n",
       "12                 color                            refers to color of wine   \n",
       "\n",
       "      Scale Discrete/Continuous                    Range  \n",
       "0     ratio          continuous               3.8 - 15.9  \n",
       "1     ratio          continuous              0.08 - 1.58  \n",
       "2     ratio          continuous                 0 - 1.66  \n",
       "3     ratio          continuous              0.66 - 65.8  \n",
       "4     ratio          continuous              0.01 - 0.61  \n",
       "5     ratio          continuous                  1 - 289  \n",
       "6     ratio          continuous                  6 - 440  \n",
       "7     ratio          continuous              0.99 - 1.04  \n",
       "8     ratio          continuous              2.72 - 4.01  \n",
       "9     ratio          continuous                  0.22 -2  \n",
       "10    ratio          continuous                 8 - 14.9  \n",
       "11  nominal            discrete  0: Bad, 1: OK, 2: Great  \n",
       "12  nominal            discrete         1: Red, 2: White  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe to get a nice description table\n",
    "data_des = pd.DataFrame()\n",
    "data_des['Features'] = winesDf.columns\n",
    "data_des['Description'] = ['does not evaporate readily',\n",
    "                          ' the amount of acetic acid in wine',\n",
    "                          'citric acid can add freshness and flavor to wines',\n",
    "                          'sugar remaining after fermentation stops',\n",
    "                          'the amount of salt in the wine',\n",
    "                          'prevents microbial growth and the oxidation',\n",
    "                          'SO2 becomes evident in the nose and taste of wine',\n",
    "                          'water depending on the percent alcohol and sugar content',\n",
    "                          'describes how acidic or basic a wine is',\n",
    "                          'sulfur dioxide gas (S02) levels',\n",
    "                          ' the percent alcohol content of the wine',\n",
    "                          'value given by experts on wine quality',\n",
    "                          'refers to color of wine']\n",
    "data_des['Scale'] = ['ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'nominal', 'nominal']\n",
    "data_des['Discrete/Continuous'] = ['continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'discrete', 'discrete']\n",
    "data_des['Range'] = ['3.8 - 15.9', '0.08 - 1.58', '0 - 1.66', '0.66 - 65.8','0.01 - 0.61','1 - 289','6 - 440','0.99 - 1.04','2.72 - 4.01','0.22 -2','8 - 14.9','0: Bad, 1: OK, 2: Great','1: Red, 2: White']\n",
    "data_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "0     246\n",
       "1    6053\n",
       "2     198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assign quality to 3 different classes instead of having 1-10 values\n",
    "winesDf['quality'] = winesDf['quality'].map({1: 0, 2: 0, 3: 0, 4:0 ,5: 1, 6: 1, 7:1, 8:2, 9:2, 10:2})\n",
    "winesDf.groupby('quality').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our classes are not divided evenly - there are more 'ok' wines than there are 'bad' or 'great' wines - so a wine sample would have to perform rather poorly or extraordinary to be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = winesDf['quality']\n",
    "X = winesDf.drop(['quality'], axis=1) # remove quality from X dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We originally had 2 CSV files - one for red wines and one for white wines, so we ended up merging both files and shuffling the rows to have a randomized dataset. We removed an index column since that wouldn't be necessary for our analysis. We did end up adding an extra attribute for color since we're looking at both red and white wines. We are trying to classify a quality value given 11 attributes that are fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total dioxide, density, pH, sulphates, and alcohol. These attributes tell us how likely a wine sample is to evaporate, how sweet or acid it is, how much sugar there is, the water density, pH, and how much alcohol there is in the wine. So we did not remove any of those attributes as we feel that those are required for our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# normalize data in X dataframe\n",
    "winesDfContinuous = winesDf\n",
    "winesDfContinuous = winesDfContinuous.drop(['quality', 'color'], axis=1)\n",
    "\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(winesDfContinuous)\n",
    "names = winesDfContinuous.columns\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
    "scaled_df['color'] = winesDf['color']\n",
    "X = scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Metric(s) selected and explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that precision scores seem to be a good performance metric since it will help us see if the model is predicting false positives. We want to ensure that people who use our tool are getting the true quality value for their wine. We also have an uneven class distribution so there might be a few discrepancies in our model in terms of how it is trained and we do not want to let that affect the quality value that we provide so we will normalize our data before dividing the dataset into its respective training and testing sets. \n",
    "\n",
    "In addition, we are calculating accuracy because this measure takes both true positives and true negatives into account in relation to the total results - in our case, we're trying to predict wine quality given 3 classes so we want to see how accurate our predictions are. So, we will use accuracy sort of as a sanity check and ensure that we are training/testing appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dividing data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use stratified shuffle split to divide our dataset because since we had 2 different files to read from - the white and red csv files and we randomized the rows - however we feel that we need to randomize that data even more to get an even better representation of both kinds of wines. That addresses the shuffle part, and we want to use stratified so that each fold has the same amount of each class (bad, ok, great wine) and we're able to use all 3 classes to identify a quality measure for a wine sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#select model\n",
    "clf = LogisticRegression()\n",
    "#select cross validation\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\n",
    "\n",
    "#select evaluation criteria\n",
    "my_scorer = make_scorer(f1_score, average='macro') \n",
    "\n",
    "# run model training and cross validation\n",
    "per_fold_eval_criteria = cross_val_score(estimator=clf,\n",
    "                                    X=X,\n",
    "                                    y=y,\n",
    "                                    cv=cv,\n",
    "                                    scoring=my_scorer\n",
    "                                   )\n",
    "\n",
    "plt.bar(range(len(per_fold_eval_criteria)),per_fold_eval_criteria)\n",
    "plt.ylim([min(per_fold_eval_criteria)-0.01,max(per_fold_eval_criteria)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implementation of Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi Function - Parameters\n",
    "* __(Default)__ 'sigmoid' - Sigmoid Phi Function\n",
    "* 'linear' - Linear Phi Function\n",
    "\n",
    "### Cost Function - Parameters\n",
    "* __(Default)__ - 'quadratic' - Quadratic Cost Function\n",
    "* 'entropy' - Cross Entropy Cost Function\n",
    "\n",
    "### Number of Hidden Layers\n",
    "* Able to support any number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Layer Perceptron from Class - used for inspiration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two layer perceptron base from class\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inherit from Two Layer Perceptron Base class\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        grad1 = np.zeros(W1.shape)\n",
    "        grad2 = np.zeros(W2.shape)\n",
    "        \n",
    "        # for each instance's activations \n",
    "        for (a1,a2,a3,y) in zip(A1.T,A2.T,A3.T,Y_enc.T):\n",
    "            dJ_dz2 = -2*(y - a3)*a3*(1-a3)\n",
    "            dJ_dz1 = dJ_dz2 @ W2 @ np.diag(a2*(1-a2))\n",
    "                         \n",
    "            grad2 += dJ_dz2[:,np.newaxis]  @ a2[np.newaxis,:]\n",
    "            grad1 += dJ_dz1[1:,np.newaxis] @ a1[np.newaxis,:] \n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "            \n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=200, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized two layer perceptron\n",
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "class TLPVectorizedBFGS(TwoLayerPerceptronVectorized):\n",
    "    \n",
    "    def __init__(self, gtol=1e-5, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.gtol = gtol\n",
    "        \n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pack(in1, in2):\n",
    "        '''Pack and flatten input vectors '''\n",
    "        return np.hstack((in1.flatten(),in2.flatten()))\n",
    "    \n",
    "    def _unpack(self, in_tot):\n",
    "        '''Undo packing according to layer weight sizes'''\n",
    "        out1 = in_tot[:self.W1.size].reshape(self.W1.shape)\n",
    "        out2 = in_tot[self.W1.size:].reshape(self.W2.shape)\n",
    "        return out1, out2\n",
    "    \n",
    "    def _calc_cost_gradient_packed(self,W,X_data,Y_enc):\n",
    "        '''Unpack and get cost, gradient for bfgs'''\n",
    "        W1, W2 = self._unpack(W) \n",
    "        # feedforward all instances\n",
    "        A1, Z1, A2, Z2, A3 = self._feedforward(X_data,W1,W2)\n",
    "        \n",
    "        cost = np.sum((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        cost = cost + L2_term\n",
    "        #perform back prop to get gradients\n",
    "        grad1,grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3,Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                         W1=W1, W2=W2)\n",
    "        return cost, self._pack(grad1,grad2)\n",
    "    \n",
    "    def _cost_packed(self,W,X_data,Y_enc):\n",
    "        '''Unpack and calculate MSE for bfgs'''\n",
    "        W1, W2 = self._unpack(W)\n",
    "        _, _, _, _, A3 = self._feedforward(X_data,W1,W2)\n",
    "        return np.sum((Y_enc-A3)**2)\n",
    "    \n",
    "    def fit(self,X,y,print_progress=0):\n",
    "        '''Learn weights from training data'''\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "        \n",
    "        # make initial matrices into single row vector\n",
    "        W = self._pack(self.W1,self.W2)\n",
    "        \n",
    "        if print_progress>0:\n",
    "            def callback(xd):\n",
    "                callback.counter += 1\n",
    "                if callback.counter%print_progress==0:\n",
    "                    sys.stderr.write('\\rEpoch: %d/%d (max)' % (callback.counter,callback.epochs))\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "            callback.counter = 0\n",
    "            callback.epochs = self.epochs\n",
    "            \n",
    "        else:\n",
    "            callback = None\n",
    "            \n",
    "        # compute gradient optimum with bfgs\n",
    "        W_best,_,props = fmin_l_bfgs_b(\n",
    "                        x0=W,\n",
    "                        func=self._calc_cost_gradient_packed,\n",
    "                        maxfun=self.epochs,\n",
    "                        callback=callback,\n",
    "                        pgtol=self.gtol,\n",
    "                        args=(X_data, Y_enc))\n",
    "        \n",
    "        self.W1, self.W2 = self._unpack(W_best)\n",
    "        if print_progress:\n",
    "            print(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished importing the code from class which covers a two layer perceptron. We will use this to see how the two layer perceptron performs with our dataset and then we will start building a multilayer perceptron using the above two layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4274475300004202, 0.9232307692307693, 0.5254010987528016)\n",
      "CPU times: user 46.8 s, sys: 601 ms, total: 47.4 s\n",
      "Wall time: 8.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn_bfgs = TLPVectorizedBFGS(**params, gtol=1e-3)\n",
    "\n",
    "import time\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "\n",
    "def calculate_metrics(method,X,y):\n",
    "    f1_score_sum=0.0\n",
    "    precision_sum=0.0\n",
    "    accuracy_sum=0.0\n",
    "    \n",
    "    #go through each fold and calculate metrics based on the method passed in used to predict a class\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        method.fit(X_train, y_train)\n",
    "        \n",
    "        yhat = method.predict(X_test)\n",
    "        \n",
    "        f1_score_sum+=f1_score(y_test, yhat, average='macro')\n",
    "        accuracy_sum+=accuracy_score(y_test,yhat)\n",
    "        precision_sum+=precision_score(y_test, yhat, average='macro')\n",
    "    return f1_score_sum/cv.get_n_splits(X), accuracy_sum/cv.get_n_splits(X), precision_sum/cv.get_n_splits(X)\n",
    "\n",
    "print(calculate_metrics(nn_bfgs,X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the two layer perceptron with BFGS implementation from clas, we found an accuracy of 92.323% and a precision score of 52.54%. Since the precision score is relatively low, it shows that the model still has a relatively high number of false positive predictions.\n",
    "\n",
    "* __HELP idk what else to say__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronBase(object):\n",
    "    def __init__(self, n_layers=2, n_hidden=30, C=0.0, epochs=500, \n",
    "                 eta=0.001, phi='sigmoid', cost='quadratic',random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.phi = phi\n",
    "        self.cost = cost\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y): #one hot encode labels\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self): #initialize weights with small random numbers\n",
    "        W = []\n",
    "        i = 0\n",
    "        \n",
    "        #initializing W1\n",
    "        num_elems = (self.n_features_ + 1) * self.n_hidden\n",
    "        W_temp = np.random.uniform(-1.0, 1.0, size=num_elems)\n",
    "        W_temp = W_temp.reshape(self.n_hidden, self.n_features_ + 1)\n",
    "        W.append(W_temp)\n",
    "        \n",
    "        while i < (self.n_layers - 2):\n",
    "            num_elems = (self.n_hidden) * (W[i].shape[0] + 1)\n",
    "            W_temp = np.random.uniform(-1.0, 1.0, size=num_elems)\n",
    "            W_temp = W_temp.reshape(self.n_hidden, (W[i].shape[0] + 1))\n",
    "            W.append(W_temp)\n",
    "            i += 1\n",
    "\n",
    "        #initializing last weight\n",
    "        num_elems = (self.n_output_) * (W[i].shape[0] + 1)\n",
    "        W_temp = np.random.uniform(-1.0, 1.0, size=num_elems)\n",
    "        W_temp = W_temp.reshape(self.n_output_, (W[i].shape[0] + 1))\n",
    "        W.append(W_temp)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def _activation(self, z):\n",
    "        if self.phi == 'linear':\n",
    "            return self._linear(z)\n",
    "        if self.phi == 'sigmoid':\n",
    "            return self._sigmoid(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z): #use scipy expit function to avoid overflow\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'): #add bias unit of 1s as either a column or row to array at index 0\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W): #compute L2 regularization cost\n",
    "        # only compute for non-bias terms\n",
    "        W_mean_sum = 0\n",
    "        for w in W:\n",
    "            W_mean_sum += np.mean(w[:, 1:] ** 2)\n",
    "        return (lambda_/2.0) * np.sqrt(W_mean_sum)\n",
    "    \n",
    "    def _cost(self, A_last, Y_enc, W): # get objective function value\n",
    "        if self.cost == 'quadratic':\n",
    "            cost = np.mean((Y_enc-A_last)**2)\n",
    "            L2_term = self._L2_reg(self.l2_C, W)\n",
    "        if self.cost == 'entropy':\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(A_last)+(1-Y_enc)*np.log(1-A_last))))\n",
    "            L2_term = self._L2_reg(self.l2_C, W)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(MultiLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W): #compute feedforward steps\n",
    "        A = []\n",
    "        Z = []\n",
    "        \n",
    "        A_temp = self._add_bias_unit(X.T, how='row')\n",
    "        A.append(A_temp)\n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            Z_temp = W[i] @ A[i]\n",
    "            Z.append(Z_temp)\n",
    "            A_temp = self._activation(Z[i])\n",
    "            A_temp = self._add_bias_unit(A_temp, how='row')\n",
    "            A.append(A_temp)\n",
    "            \n",
    "        Z_temp = W[self.n_layers - 1] @ A[self.n_layers - 1]\n",
    "        Z.append(Z_temp)\n",
    "        A_temp = self._activation(Z[self.n_layers - 1])\n",
    "        A.append(A_temp)\n",
    "    \n",
    "        return A, Z\n",
    "    \n",
    "    \n",
    "    def _get_gradient(self, A, Z, Y_enc, W): #compute gradient step using backpropagation\n",
    "        grad = []\n",
    "        V = []\n",
    "        \n",
    "        #calculating last sensitivity\n",
    "        V_temp = -2*(Y_enc-A[self.n_layers])*A[self.n_layers]*(1-A[self.n_layers])\n",
    "        V.append(V_temp)\n",
    "        \n",
    "        grad_temp = V[0] @ (A[self.n_layers - 1]).T\n",
    "        grad.append(grad_temp)\n",
    "        grad[0] += W[self.n_layers - 1] * self.l2_C\n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            V_temp = A[self.n_layers - i-1][1:,:] * (1-A[self.n_layers - i-1])[1:,:] * (((W[self.n_layers - i-1]).T)[1:,:] @ V[i])\n",
    "            V.append(V_temp)\n",
    "            \n",
    "            grad_temp = V[i+1] @ (A[self.n_layers - i-2]).T\n",
    "            grad_temp += (W[self.n_layers - i-2]) * self.l2_C\n",
    "            grad.append(grad_temp)\n",
    "            \n",
    "        #need to reverse the orded of the sensitivities (V) and gradients (grad)\n",
    "        #because they were appended in reverse order (V2, V1...)\n",
    "        V = V[::-1]\n",
    "        grad = grad[::-1]\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def predict(self, X): #predict class labels\n",
    "        A, Z = self._feedforward(X, self.W)\n",
    "        y_pred = np.argmax(A[self.n_layers], axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False): #learn weights from training data\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        #initialize weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.grad = []\n",
    "        self.grad_W_ = []\n",
    "        for i in range(self.n_layers):\n",
    "            self.grad_W_.append(np.zeros(self.epochs))\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A, Z = self._feedforward(X_data, self.W)\n",
    "            \n",
    "            cost = self._cost(A[self.n_layers],Y_enc,self.W)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            self.grad = self._get_gradient(A=A, Z=Z, Y_enc=Y_enc, W=self.W) \n",
    "            \n",
    "            for g in range(self.n_layers):\n",
    "                self.W[g] -= self.eta * self.grad[g]\n",
    "                self.grad_W_[g][i] = np.mean(self.grad[g]) / np.std(self.grad[g])\n",
    "                \n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "\n",
    "def calculate_metrics(method,X,y):\n",
    "    f1_score_sum=0.0\n",
    "    precision_sum=0.0\n",
    "    accuracy_sum=0.0\n",
    "    \n",
    "    #go through each fold and calculate metrics based on the method passed in used to predict a class\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        method.fit(X_train, y_train)\n",
    "        \n",
    "        yhat = method.predict(X_test)\n",
    "        \n",
    "        f1_score_sum+=f1_score(y_test, yhat, average='macro')\n",
    "        accuracy_sum+=accuracy_score(y_test,yhat)\n",
    "        precision_sum+=precision_score(y_test, yhat, average='macro')\n",
    "    return f1_score_sum/cv.get_n_splits(X), accuracy_sum/cv.get_n_splits(X), precision_sum/cv.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tuning hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our implementation is done, we will play around with different combinations of parameters to see what happens on our multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: Sigmoid & Cost: Quadratic\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  50  ->  (0.32150490035080703, 0.9314615384615385, 0.3105110637373917)\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  50  ->  (0.32150490035080703, 0.9314615384615385, 0.3105110637373917)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  90  ->  (0.3228647941091071, 0.9316153846153847, 0.3438700578354158)\n",
      " \n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  50  ->  (0.32274803562356047, 0.9313846153846155, 0.3188648306426084)\n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  50  ->  (0.32645189962203724, 0.9312307692307693, 0.3617057030837349)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      " \n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  50  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  50  ->  (0.32780071758193946, 0.9317692307692308, 0.3539624672167045)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "activation = ['sigmoid', 'linear']\n",
    "cost = ['quadratic', 'entropy']\n",
    "hidden_layers = [2, 3, 5]\n",
    "hidden_neurons = [50, 90]\n",
    "epoch = [500, 800]\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "        \n",
    "#using sigmoid activation function and quadratic cost function\n",
    "print('Activation: Sigmoid & Cost: Quadratic')\n",
    "for layer in hidden_layers:\n",
    "    for epo in epoch:\n",
    "        for neur in hidden_neurons:\n",
    "            params = dict(n_layers=layer, n_hidden=neur, C=0.1, epochs=epo, \n",
    "                      eta=0.001, phi='sigmoid', cost='quadratic', random_state=1)\n",
    "            nn = MultiLayerPerceptron(**params)\n",
    "            print('Layers: ', layer,' &  Epochs: ', epo,' &  Neurons: ', neur, ' -> ',calculate_metrics(nn,X,y))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: Sigmoid & Cost: Cross Entropy\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  50  ->  (0.32150490035080703, 0.9314615384615385, 0.3105110637373917)\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  50  ->  (0.32150490035080703, 0.9314615384615385, 0.3105110637373917)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  90  ->  (0.3228647941091071, 0.9316153846153847, 0.3438700578354158)\n",
      " \n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  50  ->  (0.32274803562356047, 0.9313846153846155, 0.3188648306426084)\n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  50  ->  (0.32645189962203724, 0.9312307692307693, 0.3617057030837349)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      " \n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  50  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  50  ->  (0.32780071758193946, 0.9317692307692308, 0.3539624672167045)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  90  ->  (0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#using sigmoid activation function and cross entropy (negative log loss) cost function\n",
    "print('Activation: Sigmoid & Cost: Cross Entropy')\n",
    "for layer in hidden_layers:\n",
    "    for epo in epoch:\n",
    "        for neur in hidden_neurons:\n",
    "            params = dict(n_layers=layer, n_hidden=neur, C=0.1, epochs=epo, \n",
    "                      eta=0.001, phi='sigmoid', cost='entropy', random_state=1)\n",
    "            nn = MultiLayerPerceptron(**params)\n",
    "            print('Layers: ', layer,' &  Epochs: ', epo,' &  Neurons: ', neur, ' -> ',calculate_metrics(nn,X,y))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: Linear & Cost: Quadratic\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      " \n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      " \n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#using linear activation function and quadratic cost function\n",
    "print('Activation: Linear & Cost: Quadratic')\n",
    "for layer in hidden_layers:\n",
    "    for epo in epoch:\n",
    "        for neur in hidden_neurons:\n",
    "            params = dict(n_layers=layer, n_hidden=neur, C=0.1, epochs=epo, \n",
    "                      eta=0.001, phi='linear', cost='quadratic', random_state=1)\n",
    "            nn = MultiLayerPerceptron(**params)\n",
    "            print('Layers: ', layer,' &  Epochs: ', epo,' &  Neurons: ', neur, ' -> ',calculate_metrics(nn,X,y))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: Linear & Cost: Cross Entropy\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  2  &  Epochs:  500  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  2  &  Epochs:  800  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      " \n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  3  &  Epochs:  500  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  3  &  Epochs:  800  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      " \n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  5  &  Epochs:  500  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  50  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      "Layers:  5  &  Epochs:  800  &  Neurons:  90  ->  (0.024215468248085004, 0.03769230769230769, 0.012564102564102564)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#using linear activation function and cross entropy (negative log loss) cost function\n",
    "print('Activation: Linear & Cost: Cross Entropy')\n",
    "for layer in hidden_layers:\n",
    "    for epo in epoch:\n",
    "        for neur in hidden_neurons:\n",
    "            params = dict(n_layers=layer, n_hidden=neur, C=0.1, epochs=epo, \n",
    "                      eta=0.001, phi='linear', cost='entropy', random_state=1)\n",
    "            nn = MultiLayerPerceptron(**params)\n",
    "            print('Layers: ', layer,' &  Epochs: ', epo,' &  Neurons: ', neur, ' -> ',calculate_metrics(nn,X,y))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tests above, we found the best results, according to our performance metric, precision, to be the combination of the following parameters:\n",
    "* Activation Function: __Sigmoid__\n",
    "* Cost Function: __Quadratic__\n",
    "* Number of Layers: __3__\n",
    "* Number of Hidden Neurons: __50__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy produced from our model was also 93.138%, which shows that our model is pretty good a making correct predictions - meaning it's good a saying what a wine's quality might be or might not be. But this metric does not show how well our model can predict what a wine's quality might be. This is why we will also look at the precision score. The precision score from our model was 36.918%. This is not a high number, which indicates that the model predicted a high number of false positives - meaning it predicted that a wine's quality was 'ok' or 'gread' but that specific wine's quality was actually 'bad'. \n",
    "\n",
    "* __HELP - should we try to explain why the precision score was so low? and maybe other techniques that could help make it better? was it overtrained should we used dropout so there is no memorization? more layers? different gradient calculation (bfgs from above)?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualizing magnitude of gradients in  each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3297995488109412, 0.9313846153846154, 0.36918360519636007)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "params = dict(n_layers=3, n_hidden=50, C=0.001, epochs=800, \n",
    "                      eta=0.001, phi='sigmoid', cost='quadratic', random_state=1)\n",
    "nn = MultiLayerPerceptron(**params)\n",
    "print(calculate_metrics(nn,X,y))\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "for i in range(len(nn.grad)):\n",
    "    plt.plot(abs(nn.grad_W_[i][10:]), label=('w'+str(i)))\n",
    "    \n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, we are able to see the magnitude of the gradients of each layer and how they changed throughout the number of epochs. You can see that the gradient in the first layer (w0) is smaller than the other gradients, which makes sense because each gradients and sensitivity is affected by the later ones, which are likely to change more. The larger magnitudes of the gradients, like w1 and w2 show that the weights in that layer are changing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparing our MLP to Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.3\n",
      "(0.32151865126775514, 0.9315384615384616, 0.31051282051282053)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(sklearn_version)\n",
    "# these values have been hand tuned\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50, ), \n",
    "                    activation='logistic', \n",
    "                    solver='sgd', \n",
    "                    alpha=0.001, # L2 penalty\n",
    "                    batch_size='auto', # min of 200, num_samples\n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.001, # only SGD\n",
    "                    power_t=0.5,    # only SGD\n",
    "                    max_iter=75, \n",
    "                    shuffle=True, \n",
    "                    random_state=1, \n",
    "                    tol=1e-9, # for stopping\n",
    "                    verbose=False, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, # only SGD\n",
    "                    nesterovs_momentum=True, # only SGD\n",
    "                    early_stopping=False, \n",
    "                    validation_fraction=0.1, # only if early_stop is true\n",
    "                    beta_1=0.9, # adam decay rate of moment\n",
    "                    beta_2=0.999, # adam decay rate of moment\n",
    "                    epsilon=1e-08) # adam numerical stabilizer\n",
    "\n",
    "print(calculate_metrics(clf,X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn model above uses mostly the parameters from our implementation of a multi-layer perceptron and the final result, meaning the accuracy and the precision score, are very similar. Our model (with the hyper parameters with the best results) produced an accuracy of 93.138% and sklearns model produced an accuracy of 93.153%. Our model also produced a precision of 36.918% and sklearns model produced an accuracy of 31.051%. When it comes to predicting wine quality, either our implementation or sklearns implementation could be used since the results are relatively similar. However, our implementation produced a higher precision score, meaning it predicted a smaller amoung of false positives. Due to this, our implementation may be the better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We completed the biometrics research study session with Elena Sharp in the Johnson Square building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Wine Quality Dataset. Kaggle. https://www.kaggle.com/danielpanizzo/wine-quality\n",
    "2. Dylan Sprouse Is The CEO Of All-Wise Meadery & It's The Career He's Always Wanted. Amanda Fama. August 13, 2018. https://www.elitedaily.com/p/dylan-sprouse-is-the-ceo-of-all-wise-meadery-its-the-career-hes-always-wanted-9979370"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
